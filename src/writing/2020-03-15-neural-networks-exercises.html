---
title: "Neural Networks and Deep Learning: Exercises"
math: true
---

<p>Here you can find my solutions to the exercises in Michael Nielsen’s free online book <cite><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></cite>.</p>

<p>I decided to do those exercises out in the open to force myself to, well, <em>do</em> them but also prove to myself that I actually understood the material. I have a personal and professional interest in neural networks and I want to get it right. That being said, there is no guarantee that I will succeed in this endeavour and I by no means claim to have all the right answers. If you find any mistakes, please consider raising an issue on the GitHub repository of this blog.</p>

<p>Anyway, onwards!</p>

[h level=2]Perceptrons and Sigmoid Neurons[/h]

[h level=3]Sigmoid Neurons Simulating Perceptrons, Part I[/h]

<details>
  <summary>Exercise</summary>
  <i>Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, \(c>0\). Show that the behaviour of the network doesn't change.</i>
</details>

<p>Before I solve this exercise, I want to recap what we have learned so far. A <i>perceptron</i> is a type of an artificial neuron that looks like this:</p>

<svg xmlns="http://www.w3.org/2000/svg" viewBox="-68.7 -72 119.1 28.7"><g id="page1"><path d="M-.6-57.6c0-7.9-6.4-14.2-14.2-14.2-7.8 0-14.2 6.3-14.2 14.2 0 7.8 6.4 14.1 14.2 14.1C-7-43.5-.6-49.8-.6-57.6z" stroke="#000" fill="none" stroke-width=".4"/><text class="f0" x="-14.8" y="-57.6" transform="translate(-2.1 2.4)">b</text><text class="f0" x="-14.8" y="-57.6" transform="translate(-53.9 2.4)">x</text><text class="f0" x="-14.8" y="-57.6" transform="translate(36.8 2.4)">output</text><path d="M-59.7-57.6h30.1" stroke="#000" fill="none" stroke-width=".4" stroke-miterlimit="10"/><path d="M-30.8-59.2c.1.6 1.2 1.5 1.5 1.6-.3.1-1.4 1-1.5 1.6" stroke="#000" fill="none" stroke-width=".3" stroke-miterlimit="10" stroke-linecap="round" stroke-linejoin="round"/><text class="f0" x="-14.8" y="-57.6" transform="translate(-33.3 -3.5)">w</text><path d="M-.4-57.6h18.6" stroke="#000" fill="none" stroke-width=".4" stroke-miterlimit="10"/><path d="M17-59.2c.1.6 1.2 1.5 1.5 1.6-.3.1-1.4 1-1.5 1.6" stroke="#000" fill="none" stroke-width=".3" stroke-miterlimit="10" stroke-linecap="round" stroke-linejoin="round"/></g></svg>

<p>A perceptron takes an input vector \(x\), a weight vector \(w\), and a bias \(b\) and produces a single binary \(output\) where</p>

\begin{align}
output = \begin{cases}
0 &\quad w \cdot x + b \leq 0\\
1 &\quad w \cdot x + b > 0\\
\end{cases}
\end{align}

<p>To show the above, I will show that the behavior of one single perceptron will not change if we multiply its weight vector and bias with a positive constant \(c\).</p>

<p>The output of the perceptron would be:</p>

\begin{align}
output(c) &= \begin{cases}
0 &\quad c(w \cdot x) + cb \leq 0\\
1 &\quad c(w \cdot x) + cb > 0
\end{cases}\\
&= \begin{cases}
0 &\quad c(w \cdot x + b) \leq 0\\
1 &\quad c(w \cdot x + b) > 0
\end{cases}\\
&= \begin{cases}
0 &\quad w \cdot x + b \leq 0\\
1 &\quad w \cdot x + b > 0
\end{cases}
\end{align}

<p>I first used the <i>distributive law</i> and then the fact that a positive number does not change the sign of a multiplication. Thus we arrived at the definition of a perceptron’s output. Since the behavior of a single perceptron does not change by the introduction of \(c\), the behavior of a neural network as a whole would not change.</p>

[h level=3]Sigmoid Neurons Simulating Perceptrons, Part II[/h]

<details>
  <summary>Exercise</summary>
  <i>Suppose we have the same setup as the last problem—a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that \(w \cdot x + b \neq 0\) for the input \(x\) to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant \(c > 0\). Show that in the limit as \(c \to \infty\) the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when \(w \cdot x + b = 0\) for one of the perceptrons? </i>
</details>

<p>A <i>sigmoid neuron</i> looks exactly like a perceptron but the output now computes as \(output = \sigma(w \cdot x + b)\) where \(\sigma\) is called the <i>sigmoid function</i>:</p>

\begin{align}
\sigma(z) \equiv \frac{1}{1 + e^{-z}}
\end{align}

<p>In particular that means that the output can be any value <em>between</em> \(0\) and \(1\) instead of <em>either</em> \(0\) or \(1\).</p>

<p>As in the exercise above, let’s assess the behavior of a single sigmoid neuron when weight and bias are multiplied with a positive \(c\):</p>

\begin{align}
output(c) &= \frac{1}{1 + e^{-(cw \cdot x + cb)}}\\
&= \frac{1}{1 + e^{-c(w \cdot x + b)}}
\end{align}

<p>As \(c\) approaches infinity we get:</p>

\begin{align}
\lim_{c \to \infty}{output(c)} &= \begin{cases}
\frac{1}{1 + \infty} &= 0 &\quad w \cdot x + b < 0\\
\frac{1}{1 + 0} &= 1 &\quad w \cdot x + b > 0
\end{cases}
\end{align}

<p>That is exactly the same behavior a perceptron would have for \(w \cdot x + b \neq 0\). Since this is true for one single sigmoid neuron, it also holds true for a full network of those neurons.</p>

<p>For \(w \cdot x + b = 0\), on the other hand, we get:</p>

\begin{align}
\lim_{c \to \infty}{output(c)} &= \frac{1}{1 + e^{0}}\\
&= 0.5
\end{align}

<p>A perceptron would output \(0\) here.</p>

<hr>

<p>To be continued…</p>