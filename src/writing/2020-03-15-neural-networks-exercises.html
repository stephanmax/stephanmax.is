---
title: "Neural Networks and Deep Learning: Exercises"
css: [math]
---

<p>Here you can find my solutions to the exercises in Michael Nielsen’s free online book <cite><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></cite>.</p>

<p>I decided to do those exercises out in the open to force myself to, well, <em>do</em> them but also prove to myself that I actually understood the material. I have a personal and professional interest in neural networks and I want to get it right. That being said, there is no guarantee that I will succeed in this endeavour and I by no means claim to have all the right answers. If you find any mistakes, please consider raising an issue on the GitHub repository of this blog.</p>

<p>Anyway, onwards!</p>

[h level=2]Perceptrons and Sigmoid Neurons[/h]

<p><a href="http://neuralnetworksanddeeplearning.com/chap1.html#exercises_191892">Exercise link</a></p>

[h level=3]Sigmoid Neurons Simulating Perceptrons, Part I[/h]

<details>
  <summary>Exercise</summary>
  <i>Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, \(c>0\). Show that the behaviour of the network doesn't change.</i>
</details>

<p>Before I solve this exercise, I want to recap what we have learned so far. A <i>perceptron</i> is a type of an artificial neuron that looks like this:</p>

[svg width="15rem"]neural-networks-exercises/perceptron[/svg]

<p>A perceptron takes an input vector \(x\), a weight vector \(w\), and a bias \(b\) and produces a single binary \(output\) where</p>

\begin{align}
output = \begin{cases}
0 &\quad w \cdot x + b \leq 0\\
1 &\quad w \cdot x + b > 0\\
\end{cases}
\end{align}

<p>To show the above, I will show that the behavior of one single perceptron will not change if we multiply its weight vector and bias with a positive constant \(c\).</p>

<p>The output of the perceptron would be:</p>

\begin{align}
output(c) &= \begin{cases}
0 &\quad c(w \cdot x) + cb \leq 0\\
1 &\quad c(w \cdot x) + cb > 0
\end{cases}\\
&= \begin{cases}
0 &\quad c(w \cdot x + b) \leq 0\\
1 &\quad c(w \cdot x + b) > 0
\end{cases}\\
&= \begin{cases}
0 &\quad w \cdot x + b \leq 0\\
1 &\quad w \cdot x + b > 0
\end{cases}
\end{align}

<p>I first used the <i>distributive law</i> and then the fact that a positive number does not change the sign of a multiplication. Thus we arrived at the definition of a perceptron’s output. Since the behavior of a single perceptron does not change by the introduction of \(c\), the behavior of a neural network as a whole would not change.</p>

[h level=3]Sigmoid Neurons Simulating Perceptrons, Part II[/h]

<details>
  <summary>Exercise</summary>
  <i>Suppose we have the same setup as the last problem—a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that \(w \cdot x + b \neq 0\) for the input \(x\) to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant \(c > 0\). Show that in the limit as \(c \to \infty\) the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when \(w \cdot x + b = 0\) for one of the perceptrons? </i>
</details>

<p>A <i>sigmoid neuron</i> looks exactly like a perceptron but the output now computes as \(output = \sigma(w \cdot x + b)\) where \(\sigma\) is called the <i>sigmoid function</i>:</p>

\begin{align}
\sigma(z) \equiv \frac{1}{1 + e^{-z}}
\end{align}

<p>In particular that means that the output can be any value <em>between</em> \(0\) and \(1\) instead of <em>either</em> \(0\) or \(1\).</p>

<p>As in the exercise above, let’s assess the behavior of a single sigmoid neuron when weight and bias are multiplied with a positive \(c\):</p>

\begin{align}
output(c) &= \frac{1}{1 + e^{-(cw \cdot x + cb)}}\\
&= \frac{1}{1 + e^{-c(w \cdot x + b)}}
\end{align}

<p>As \(c\) approaches infinity we get:</p>

\begin{align}
\lim_{c \to \infty}{output(c)} &= \begin{cases}
\frac{1}{1 + \infty} &= 0 &\quad w \cdot x + b < 0\\
\frac{1}{1 + 0} &= 1 &\quad w \cdot x + b > 0
\end{cases}
\end{align}

<p>That is exactly the same behavior a perceptron would have for \(w \cdot x + b \neq 0\). Since this is true for one single sigmoid neuron, it also holds true for a full network of those neurons.</p>

<p>For \(w \cdot x + b = 0\), on the other hand, we get:</p>

\begin{align}
\lim_{c \to \infty}{output(c)} &= \frac{1}{1 + e^{0}}\\
&= 0.5
\end{align}

<p>A perceptron would output \(0\) here.</p>

[h level=2]A Simple Network to Classify Handwritten Digits[/h]

<p><a href="http://neuralnetworksanddeeplearning.com/chap1.html#exercise_513527">Exercise link</a></p>

<details>
  <summary>Exercise</summary>
  <i>There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least \(0.99\), and incorrect outputs have activation less than \(0.01\).</i>
</details>

<p>I quickly came up with the weight vectors for each of the four output neurons where \(w_1\) is for the neuron representing the <i>most significant bit</i> (MSB) and \(w_4\) is for the neuron representing the <i>least significant bit</i> (LSB).</p>

\begin{align}
w_1 &= \begin{pmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1\end{pmatrix} &
w_2 &= \begin{pmatrix}0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \\ 1 \\ 1 \\ 0 \\ 0\end{pmatrix} &
w_3 &= \begin{pmatrix}0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0\end{pmatrix} &
w_4 &= \begin{pmatrix}0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1\end{pmatrix}
\end{align}

<p>Consider for example the numbers 8 and 9 which are <mark>1</mark>000 and <mark>1</mark>001 in binary notation. The yellow bits are the MSBs and hence vector \(w_1\) needs a \(1\) in position \(8\) and \(9\) to activate this bit. The weighted input \(w_1 \cdot x\) is then [sidenote label="approximately"]To be precise we have \(0.99 \leq w_1 \cdot x < 1.01\) for \(8\) or \(9\) and \(0 \leq w_1 \cdot x < 0.02\) for other digits.[/sidenote] \(1\) for \(8\) and \(9\) and approximately \(0\) for other digits. The other digit inputs and weights work accordingly. That is a great start.</p>

<p>Now we have to figure out which bias to choose for each neuron. Let’s have a look at the sigmoid function for that:</p>

[svg width="20rem"]neural-networks-exercises/sigmoid_function[/svg]

<p>The sigmoid function centers around \(0\) and with \(b = -0.5\) we shift the activation for an output that “sets a bit” (where \(w \cdot x \approx 1\)) to \(\sigma(w \cdot x + b) \approx \sigma(0.5) \approx 0.62\). Similarly, the activation of a neuron that signals an “unset bit” (where \(w \cdot x \approx 0\)) would output \(\sigma(w \cdot x + b) \approx \sigma(-0.5) \approx 0.38\).</p>

<p>We could now classify a neuron output greater than \(0.5\) as “this bit is set” and an output less than \(0.5\) as “this bit is not set” and be done with it. But let’s try to find the factor that gives us the same precision that the neurons in the hidden layers have.</p>

<p>We are looking for a factor \(c\) such that an output of \(\sigma(c(w \cdot x + b)) \geq 0.99\) “sets a bit” and at the same time an output of \(\sigma(c(w \cdot x + b)) < 0.01\) signals an “unset bit”.</p>

<p>Let’s consider the case of a set bit first. We can set \(w \cdot x = 0.99\) since this is the minimum value possible based on the precision of the old output layer and a bigger value would just increase the value of \(\sigma(\cdot)\).</p>

\begin{align}
0.99 &\leq \sigma(c(w \cdot x + b))\\
0.99 &\leq \sigma(c(0.99 - 0.5))\\
0.99 &\leq \frac{1}{1 + e^{-c(0.99 - 0.5)}}\\
0.99 &\leq \frac{1}{1 + e^{-0.49c}}\\
1 + e^{-0.49c} &\leq \frac{1}{0.99}\\
e^{-0.49c} &\leq \frac{0.01}{0.99}\\
e^{0.49c} &\geq 99\\
0.49c &\geq \ln{99}\\
c &\geq \frac{\ln{99}}{0.49} \approx 9.38\\
\end{align}

<p>Now I will look at the output for an unset bit where I we want to achieve an activation of less than \(0.01\). We can set \(w \cdot x = 0.05\) since across our four weight vectors we have a maximum of five contributing \(1\)s in \(w_4\) (and each \(x_i\) has an activation of less than \(0.01\) according to the precision of the old output layer in the exercise text) and:</p>

\begin{align}
w_4 \cdot x &= \begin{pmatrix}0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1 \\ 0 \\ 1\end{pmatrix} \cdot \begin{pmatrix}x_0 \\ x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8 \\ x_9\end{pmatrix}\\
&= x_1 + x_3 + x_5 + x_7 + x_9\\
&\leq 5 \cdot 0.01 = 0.05
\end{align}

<p>That gives us a \(c\) which:</p>

\begin{align}
0.01 &> \sigma(c(w \cdot x + b))\\
0.01 &> \sigma(c(0.05 - 0.5))\\
0.01 &> \frac{1}{1 + e^{-c(0.05 - 0.5)}}\\
0.01 &> \frac{1}{1 + e^{0.45c}}\\
1 + e^{0.45c} &> 100\\
e^{0.45c} &> 99\\
0.45c &> \ln{99}\\
c &> \frac{\ln{99}}{0.45} \approx 10.21\\
\end{align}

<p>With no loss of generality, I choose \(c = 11\) (because it looks better than some crooked number). That gives us a bias of \(b = -0.5 \cdot 11 = -5.5\) and those weight vectors:</p>

\begin{align}
w_1 &= \begin{pmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 11 \\ 11\end{pmatrix} &
w_2 &= \begin{pmatrix}0 \\ 0 \\ 0 \\ 0 \\ 11 \\ 11 \\ 11 \\ 11 \\ 0 \\ 0\end{pmatrix} &
w_3 &= \begin{pmatrix}0 \\ 0 \\ 11 \\ 11 \\ 0 \\ 0 \\ 11 \\ 11 \\ 0 \\ 0\end{pmatrix} &
w_4 &= \begin{pmatrix}0 \\ 11 \\ 0 \\ 11 \\ 0 \\ 11 \\ 0 \\ 11 \\ 0 \\ 11\end{pmatrix}
\end{align}

[h level=2]Learning with Gradient Descent[/h]

<p><a href="http://neuralnetworksanddeeplearning.com/chap1.html#exercises_647181">Exercise link</a></p>

<p>If you need a bit more intuition and visuals around gradient decent I can recommend <cite><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">Gradient descent, how neural networks learn</a></cite> by 3Blue1Brown and the lectures 2.5, 2.6, and 2.7 of Andrew Ng’s <cite><a href="https://www.youtube.com/watch?v=F6GSRDoB-Cg&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=8">Machine Learning</a> course.</cite></p>

[h level=3]Optimality of Gradient Decent[/h]

<details>
  <summary>Exercise</summary>
  <i>Prove the assertion of the last paragraph. Hint: If you're not already familiar with the <a href="http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy-Schwarz inequality</a>, you may find it helpful to familiarize yourself with it.</i>
</details>

<p>Let’s recap what this chapter is all about. Please note that I will tackle this topic in a general manner, i.e. for the moment we can forget about neural networks, weights, and biases. All we need to know is that we have a smooth cost function \(C(v)\) where \(v\) is the input vector. We want to minimize \(C(v)\) and use <i>gradient decent</i> to change \(v\) step by step such that it approaches a local minimum of \(C\).</p>

<p>Fortunately, smart people have shown that a function’s <i>gradient</i> \(\nabla C\) gives you the direction of <i>steepest ascent</i> and the change in a function can be approximated by the inner product of the change in the input vector and that [sidenote label="gradient"]I won’t go into the nature of this gradient or how it can be computed. All that matters for me right now is that there <em>exists</em> a way to compute \(\nabla C\).[/sidenote]:</p>

\begin{align}
\Delta C &\approx \nabla C\cdot \Delta v
\end{align}

<p>If we choose \(\Delta v = -\eta \nabla C\) with \(\eta > 0\) we get:</p>

\begin{align}
\Delta C &\approx \nabla C\cdot \Delta v\\
&= -\eta \nabla C \nabla C\\
&= -\eta \|\nabla C\|^2
\end{align}

<p>This is great! Since \(\|\nabla C\|^2 \geq 0\) and \(\eta > 0\) this guarantees that \(\Delta C \leq 0\), i.e. \(C\) will always decrease.</p>

<p>To put that more visually, we take the direction \(\nabla C\) that <i>increases</i> \(C\) most quickly and make sure we go exactly the opposite way to <i>decrease</i> \(C\) most quickly. The exercise now is to show that the direction of the negative gradient actually gives us the “biggest” decrease possible.</p>

<p>Let’s revisit the equation for the approximate change in \(C\) with \(\eta = \epsilon / \|\nabla C\|\):</p>

\begin{align}
\Delta C &\approx \nabla C\cdot \Delta v\\
&= -\eta \nabla C \nabla C\\
&= -\epsilon \frac{\nabla C \nabla C}{\|\nabla C\|}\\
&= -\epsilon \|\nabla C\|\\
\end{align}

<p>Now we consider another vector \(d\) such that \(d \neq \nabla C\) and \(\|d\| = \|\nabla C\| = \epsilon\). In particular, that means that \(\|d\| / \epsilon = 1\). We can use that in the equation above:</p>

\begin{align}
\nabla C\cdot \Delta v &= -\epsilon \|\nabla C\|\\
&= -\epsilon \|\nabla C\| \frac{\|d\|}{\epsilon}\\
&= -\|\nabla C\| \|d\|\\
&\leq \nabla C \cdot d\\
\end{align}

<p>The last line is explained by the Cauchy-Schwarz inequation that states that \(|a\cdot b| \leq \|a\|\|b\|\) which implies that \(-a\cdot b \leq \|a\|\|b\|\), or \(a\cdot b \geq -\|a\|\|b\|\).</p>

<p>And there we have it: We have shown that \(\nabla C\cdot \Delta v \leq \nabla C \cdot d\). That means the change \(\Delta v = -\eta \nabla C\) gives us a “bigger” decrease in \(C\) than any other vector \(d\).</p>

<hr>

<p>To be continued…</p>